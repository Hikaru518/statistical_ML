{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis\n",
    "In this exercise, you will use principal component analysis (PCA) to perform\n",
    "dimensionality reduction. You will first experiment with an example 2D\n",
    "dataset to get intuition on how PCA works, and then use it on a bigger\n",
    "dataset of 5000 faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import utils_pca\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on a simple 2D example\n",
    "To help you understand how PCA works, you will first start with a 2D dataset\n",
    "which has one direction of large variation and one of smaller variation. The\n",
    "cell below will plot the training data (Figure 3 of your homework PDF). In this part of the\n",
    "exercise, you will visualize what happens when you use PCA to reduce the\n",
    "data from 2D to 1D. In practice, you might want to reduce data from 256 to\n",
    "50 dimensions, say; but using lower dimensional data in this example allows\n",
    "us to visualize the algorithms better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = scipy.io.loadmat('pcadata1.mat')\n",
    "X = data['X']\n",
    "plt.scatter(X[:,0],X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5.1: Implementing PCA (5 points)}\n",
    "\n",
    "In this part of the exercise, you will implement PCA. PCA consists of\n",
    "two computational steps: First, you compute the covariance matrix of the\n",
    "data. Then, you use **numpy's** SVD function to compute the eigenvectors\n",
    "$U_1, U_2, \\ldots , U_n$. These will correspond to the principal components of variation\n",
    "in the data.\n",
    "\n",
    "Before using PCA, it is important to first normalize the data by subtracting\n",
    "the mean value of each feature from the dataset, and scaling each dimension\n",
    "so that they are in the same range. The notebook {\\tt pca.ipynb},\n",
    "does this normalization for you using the {\\tt feature\\_normalize}\n",
    "function.\n",
    "After normalizing the data, you can run PCA to compute the principal\n",
    "components. You task is to complete the function **pca** in **utils_pca.py** to compute the principal\n",
    "components of the dataset. First, you should compute the covariance\n",
    "matrix of the data, which is given by:\n",
    "\n",
    "$$ \\Sigma = \\frac{1}{m} X^T X $$\n",
    "\n",
    "where $X$ is the data matrix with examples in rows, and $m$ is the number of\n",
    "examples. Note that $\\Sigma$ is a $d \\times d$ matrix and not the summation operator.\n",
    "\n",
    "After computing the covariance matrix, you can run SVD on it to compute\n",
    "the principal components. In **numpy**, you can run SVD with the following\n",
    "command: \n",
    "\n",
    "$$U,S,V = np.linalg.svd(Sigma,full\\_matrices = False)$$\n",
    "\n",
    "where **U** will contain the principal\n",
    "components and **S** will contain a diagonal matrix.\n",
    "\n",
    "Once you have completed the function, the cell below will run PCA on\n",
    "the example dataset and plot the corresponding principal components found\n",
    "(Figure 4 of your homework PDF). The cell will also output the top principal component (eigenvector)\n",
    "found, and you should expect to see an output of about [-0.707\n",
    "-0.707]. (It is possible that **numpy** may instead output the negative of this,\n",
    "since $U_1$ and $-U_1$ are equally valid choices for the first principal component.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils_pca\n",
    "\n",
    "# You should now implement the function pca in utils_pca.py\n",
    "\n",
    "# First, normalize X\n",
    "Xnorm, mu, sigma = utils_pca.feature_normalize(X)\n",
    "# Then run, PCA\n",
    "U,S,V = utils_pca.pca(Xnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize the eigenvectors\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "eig1 = mu + 1.5*S[0]*U[0,:]\n",
    "eig2 = mu + 1.5*S[1]*U[1,:]\n",
    "plt.plot([mu[0],eig1[0]],[mu[1],eig1[1]],'r')\n",
    "plt.plot([mu[0],eig2[0]],[mu[1],eig2[1]],'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction with PCA\n",
    "\n",
    "After computing the principal components, you can use them to reduce the\n",
    "feature dimension of your dataset by projecting each example onto a lower\n",
    "dimensional space, $x^{(i)} \\rightarrow  z^{(i)}$ (e.g., projecting the data from 2D to 1D). In\n",
    "this part of the exercise, you will use the eigenvectors returned by PCA and\n",
    "project the example dataset into a 1-dimensional space.\n",
    "In practice, if you were using a learning algorithm such as linear regression\n",
    "or perhaps neural networks, you could now use the projected data instead\n",
    "of the original data. By using the projected data, you can train your model\n",
    "faster as there are fewer dimensions in the input.\n",
    "\n",
    "## Problem 5.2: Projecting the data onto the principal components (5 points)\n",
    "You should now complete the function **project_data** in ** utils_pca.py**. Specifically, you are\n",
    "given a dataset **X**, the principal components **U**, and the desired number of\n",
    "dimensions to reduce to **K**. You should project each example in **X** onto the\n",
    "top **K** components in **U**. Note that the top **K** components in **U** are given by\n",
    "the first **K** columns of **U**.\n",
    "Once you have completed the function **project_data** in **utils_pca.py**, the cell below will\n",
    "project the first example onto the first dimension and you should see a value\n",
    "of about 1.481 (or possibly -1.481, if you got $-U_1$ instead of $U_1$).\n",
    "\n",
    "## Problem: 5.3 Reconstructing an approximation of the data (5 points)\n",
    "After projecting the data onto the lower dimensional space, you can approximately\n",
    "recover the data by projecting them back onto the original high\n",
    "dimensional space. Your task is to complete the function **recover_data** in **utils_pca.py** to project each\n",
    "example in **Z** back onto the original space and return the recovered approximation\n",
    "in **X_rec**. Once you have completed the function **recover_data**, the cell below will\n",
    "recover an approximation of the first example and you should see a value of\n",
    "about [-1.047 -1.047]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reconstruct the data X after projecting on the principal axis\n",
    "K = 1\n",
    "Z = utils_pca.project_data(Xnorm,U,K)\n",
    "print \"The projection of the first example (should be about 1.496) \", Z[0]\n",
    "X_rec = utils_pca.recover_data(Z,U,K)\n",
    "print \"Approximation of the first example (should be about [-1.058 -1.058]) \",X_rec[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the projections\n",
    "After completing both **project_data** and **recover_data**, the cell below will\n",
    "now perform both the projection and approximate reconstruction to show\n",
    "how the projection affects the data. In Figure 5 of the homework handout, the original data points are\n",
    "indicated with the blue circles, while the projected data points are indicated\n",
    "with the red circles. The projection effectively only retains the information\n",
    "in the direction given by $U_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the projected data\n",
    "# plot the normalized data\n",
    "plt.figure()\n",
    "plt.scatter(Xnorm[:,0],Xnorm[:,1],c='b')\n",
    "plt.ylim([-4,3])\n",
    "plt.xlim([-4,3])\n",
    "plt.axis('equal')\n",
    "plt.scatter(X_rec[:,0],X_rec[:,1],c='r')\n",
    "\n",
    "# draw lines connecting the projected points to the original points\n",
    "for i in range(X_rec.shape[0]):\n",
    "    plt.plot([Xnorm[i,0],X_rec[i,0]],[Xnorm[i,1],X_rec[i,1]],'b',linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face image dataset\n",
    "\n",
    "In this part of the exercise, you will run PCA on face images to see how it\n",
    "can be used in practice for dimension reduction. The dataset **pcafaces.mat**\n",
    "contains a dataset **X** of face images, each $32 \\times 32$ in grayscale. Each row\n",
    "of **X**  corresponds to one face image (a row vector of length 1024). The next\n",
    "cell will load and visualize the first 100 of these face images\n",
    "(Figure 6 of homework PDF).\n",
    "\n",
    "## PCA on faces\n",
    "To run PCA on the face dataset, we first normalize the dataset by subtracting\n",
    "the mean of each feature from the data matrix **X**. The cell below will\n",
    "do this for you and then run your PCA function. After running PCA, you will\n",
    "obtain the principal components of the dataset. Notice that each principal\n",
    "component in U (each row) is a vector of length $d$ (where for the face dataset,\n",
    "$d = 1024$). It turns out that we can visualize these principal components by\n",
    "reshaping each of them into a $32\\times 32$ matrix that corresponds to the pixels\n",
    "in the original dataset. The cell below displays the first 25 principal\n",
    "components that describe the largest variations (Figure 7 of homework PDF). If you want, you\n",
    "can also change the code below  to display more principal components to see how\n",
    "they capture more and more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA on the faces data set\n",
    "faces_data = scipy.io.loadmat('pcafaces.mat')\n",
    "faces = faces_data['X']\n",
    "\n",
    "# visualize the first 25 faces\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(faces[i,:].reshape((32,32)).T)\n",
    "    \n",
    "# normalize the faces data set\n",
    "faces_norm, mu_faces, sigma_faces = utils_pca.feature_normalize(faces)\n",
    "\n",
    "# run PCA\n",
    "U_faces, S_faces, V_faces = utils_pca.pca(faces_norm)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(S_faces)\n",
    "\n",
    "plt.figure()\n",
    "# top 25 eigenfaces\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(U_faces[:,i].reshape((32,32)).T)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "Now that you have computed the principal components for the face dataset,\n",
    "you can use it to reduce the dimension of the face dataset. This allows you to\n",
    "use your learning algorithm with a smaller input size (e.g., 100 dimensions)\n",
    "instead of the original 1024 dimensions. This can help speed up your learning\n",
    "algorithm.\n",
    "\n",
    "The next cell will project the face dataset onto only the\n",
    "first 100 principal components. Concretely, each face image is now described\n",
    "by a vector $z^{(i)} \\in \\Re^{100}$. \n",
    "To understand what is lost in the dimension reduction, you can recover\n",
    "the data using only the projected dataset. An approximate\n",
    "recovery of the data is performed and the original and projected face images\n",
    "are displayed side by side (Figure 8). From the reconstruction, you can observe\n",
    "that the general structure and appearance of the face are kept while\n",
    "the fine details are lost. This is a remarkable reduction (more than $10 \\times$) in\n",
    "the dataset size that can help speed up your learning algorithm significantly.\n",
    "For example, if you were training a neural network to perform person recognition\n",
    "(given a face image, predict the identitfy of the person), you can use\n",
    "the dimension reduced input of only a 100 dimensions instead of the original\n",
    "pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show the recovered faces constructed out of top 100 principal components\n",
    "K = 100\n",
    "Z_faces = utils_pca.project_data(faces_norm,U_faces,K)\n",
    "faces_rec = utils_pca.recover_data(Z_faces,U_faces,K)\n",
    "\n",
    "plt.figure()\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(faces_rec[i,:].reshape((32,32)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
