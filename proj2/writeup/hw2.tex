\documentclass[]{book}

%These tell TeX which packages to use.
\usepackage{array,epsfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{color}

%Here I define some theorem styles and shortcut commands for symbols I use often
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem*{rmk}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem*{joke}{Joke}
\newtheorem{ex}{Example}
\newtheorem*{soln}{Solution}
\newtheorem{prop}{Proposition}

\newcommand{\lra}{\longrightarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\surj}{\twoheadrightarrow}
\newcommand{\graph}{\mathrm{graph}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\Z}{\bb{Z}}
\newcommand{\Q}{\bb{Q}}
\newcommand{\R}{\bb{R}}
\newcommand{\C}{\bb{C}}
\newcommand{\N}{\bb{N}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\MM}{\mathscr{M}}
\newcommand{\HH}{\mathscr{H}}
\newcommand{\Om}{\Omega}
\newcommand{\Ho}{\in\HH(\Om)}
\newcommand{\bd}{\partial}
\newcommand{\del}{\partial}
\newcommand{\bardel}{\overline\partial}
\newcommand{\textdf}[1]{\textbf{\textsf{#1}}\index{#1}}
\newcommand{\img}{\mathrm{img}}
\newcommand{\ip}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\inter}[1]{\mathrm{int}{#1}}
\newcommand{\exter}[1]{\mathrm{ext}{#1}}
\newcommand{\cl}[1]{\mathrm{cl}{#1}}
\newcommand{\ds}{\displaystyle}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\cnt}{\mathrm{ct}}
\newcommand{\osc}{\mathrm{osc}}
\newcommand{\LL}{\mathbf{L}}
\newcommand{\UU}{\mathbf{U}}
\newcommand{\support}{\mathrm{support}}
\newcommand{\AND}{\;\wedge\;}
\newcommand{\OR}{\;\vee\;}
\newcommand{\Oset}{\varnothing}
\newcommand{\st}{\ni}
\newcommand{\wh}{\widehat}

%Pagination stuff.
\setlength{\topmargin}{-.3 in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9.in}
\setlength{\textwidth}{6.5in}
\pagestyle{empty}



\begin{document}


\begin{center}
{\Large COMP 540 \hspace{0.5cm} Homework 2}\\
\textbf{Peiguang Wang, Xinran Zhou}\\ %You should put your name here
Due: February 2, 2018 %You should write the date here.
\end{center}

\vspace{0.2 cm}

\subsection*{Part 1: Gradient and Hessian of $NLL(\theta)$ for logistic regression}
\begin{enumerate}
	\item
	Let $g(z)=\frac{1}{1-e^{-z}}$. Show that $\frac{\partial g(z)}{\partial z}=g(z)(1-g(z)).$
	\begin{proof}
		$$\frac{\partial g(z)}{\partial z} = -\frac{e^{-z}}{(1-e^{-z})} = \frac{1}{1+e^{-z}} (1 - \frac{1}{1+e^{-z}}) = g(z)(1-g(z))$$
	\end{proof}
	
	\item
	Using the previous result and the chain rule of calculus, derive the following expression for the gradient of the negative log likelihood function $NLL(\theta)$ for logistic
	regression.
	$$\frac{\partial }{\partial \theta} NNL(\theta)=\sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$
	
	\begin{proof}
		Let $h_{\theta}$ denote $h_{\theta}(x^{(i)})$. Then the $NLL(\theta)$ is:
		$$NLL(\theta) = - \frac{1}{m} \sum_{i = 1}^{m} y^{(i)} log(h_{\theta}) + (1-y^{(i)})log(1-h_{\theta})$$
		Then
		$$\frac{\partial }{\partial (\theta x) } NNL= - \sum_{i = 1}^{m} y^{(i)} \frac{h_{\theta}(1 - h_{\theta})}{h_{\theta}} + (-1)(1 - y^{(i)} \frac{h_{\theta}(1 - h_{\theta})}{1 - h_{\theta}})$$
		Simplify the equation above
		$$\frac{\partial }{\partial (\theta x) } NNL= - \sum_{i = 1}^{m} y^{(i)} (1 - h_{\theta}) + (y^{(i)} - 1) h_{\theta} = -\sum_{i = 1}^{m} y^{(i)} - h_{\theta}$$
		Derive $\frac{\partial }{\partial (\theta) } NNL $ from $\frac{\partial }{\partial (\theta x) } NNL $:
		$$\frac{\partial NNL }{\partial \theta } = \frac{\partial NLL }{\partial (\theta x) } \frac{\partial (\theta x)}{\partial \theta } = \sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$
		
	\end{proof}
	
	\item 
	The Hessian or second derivative of the $NLL(\theta)$ can be written as $H=X^{T}SX$ where
	$$S=diag(h_{\theta}(x^{(1)})(1-h_{\theta}(x^{(1)})),...,h_{\theta}(x^{(m)})(1-h_{\theta}(x^{(m)})))$$
	
	Show that $H$ is positive definite. You may assume that $0 < h_{\theta}(x^{(i)}) < 1$, so the elements of $S$ are strictly positive and that $X$ is full rank.
	
	\begin{proof}
		For any given vector $u$, compute $u^THu$:
		$$u^THu = u^TX^TSXu $$
		Let vector $v = (v_1, v_2,...v_m)$ denote $X\dot u$. since $X$ is full rank, then $v$ is not a zero vector.
		$$u^THu = v^TSv = \sum_{i=1}{m} v_i^2S_{ii}$$
		Since the elements of $S$ are strictly positive, then $u^THu > 0$. So $H$ is positive definite.
	\end{proof}
	
\end{enumerate}

\subsection*{Part 2: Properties of L2 regularized logistic regression }
\begin{enumerate}
	\item (True or False) $J(\theta)$ has multiple locally optimal solutions.
	\begin{soln}
		This statement is \textbf{False}. Since $J(\theta)$ is a convex function, it only have one global optimal point.
	\end{soln}
	
	\item (True or False) Let $\theta^{\star} = argmin_{\theta}J(\theta)$ be a global optimum. $\theta^{\star}$ is sparse (has many zero entries).
	\begin{soln}
		This statement is \textbf{False}. Since in this regression problem, we use L2 regularization, it won't make $\theta^{\star}$ become sparse. L2 norm will make $\theta$ have small values. If L1 norm is used, then $\theta^{\star}$ will become sparse.
	\end{soln}
	
	\item (True or False) If the training data is linearly separable, then some coefficients $\theta_j$ might become infinite if $\lambda = 0$.
	\begin{soln}
		This statement is \textbf{True}. When the plain is
	\end{soln}
	
	\item (True of False) The first term of $J(\theta)$ always increases as we increase $\lambda$.
	\begin{soln}
		This statement is \textbf{True}. When adding $\lambda$, the cross-entropy loss
		$$J = - \frac{1}{m} \sum_{i = 1}^{m} y^{(i)} log(h_{\theta}(x^{(i)})) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)}))$$
		will become larger to prevent overfitting.
	\end{soln}
	
\end{enumerate}

\end{document}


